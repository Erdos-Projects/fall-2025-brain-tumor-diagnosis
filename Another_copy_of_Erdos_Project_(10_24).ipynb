{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5l7cDbqKkjS",
        "outputId": "7557d00f-f3d5-4b20-bff6-208569aa80d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: monai in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install nibabel torch torchvision monai numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "CtO1bOYenISc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "shared_drives_path = \"/content/drive/Shared drives/\"\n",
        "\n",
        "if os.path.exists(shared_drives_path):\n",
        "    drives = os.listdir(shared_drives_path)\n",
        "    print(\"Available Shared Drives:\")\n",
        "    for drive_name in drives:\n",
        "        print(f\"  üìÅ {drive_name}\")\n",
        "else:\n",
        "    print(\"No Shared Drives found. Make sure you've mounted your drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1amFjIJncnp",
        "outputId": "8f1d52b3-4a6a-4bcf-9569-91242eb4f683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Available Shared Drives:\n",
            "  üìÅ Fall2025BrainTumorDiagnosis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/Shared drives/Fall2025BrainTumorDiagnosis/\""
      ],
      "metadata": {
        "id": "4pVozU1xniNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìÇ Contents of Shared Drive:\")\n",
        "for item in sorted(os.listdir(base_path)):\n",
        "    full_path = os.path.join(base_path, item)\n",
        "    if os.path.isfile(full_path):\n",
        "        print(f\"  üìÑ {item}\")\n",
        "    else:\n",
        "        print(f\"  üìÅ {item}/\")\n",
        "\n",
        "# Get all patient folders\n",
        "patient_folders = sorted([f for f in os.listdir(base_path)\n",
        "                         if os.path.isdir(os.path.join(base_path, f))])\n",
        "\n",
        "print(f\"Total patient folders: {len(patient_folders)}\")\n",
        "print(f\"\\nFirst 5 patients:\")\n",
        "for folder in patient_folders[:5]:\n",
        "    print(f\"  üìÅ {folder}\")\n",
        "\n",
        "# Check scans in the first patient\n",
        "first_patient = patient_folders[0]\n",
        "patient_path = os.path.join(base_path, first_patient)\n",
        "scans = sorted(os.listdir(patient_path))\n",
        "\n",
        "print(f\"\\nüìÇ Scans in {first_patient}:\")\n",
        "print(f\"Total scans: {len(scans)}\\n\")\n",
        "for i, scan in enumerate(scans, 1):\n",
        "    print(f\"{i}. {scan}\")\n",
        "\n",
        "# Check for label files in base directory\n",
        "print(\"\\nüìÑ Looking for label files in base directory:\")\n",
        "for item in os.listdir(base_path):\n",
        "    if item.endswith(('.csv', '.txt', '.xlsx', '.json')):\n",
        "        print(f\"  Found: {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y437qZpQnjeW",
        "outputId": "eada3672-8420-4422-8187-8c52a696d62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Contents of Shared Drive:\n",
            "  üìÅ UCSF-PDGM-0004_nifti/\n",
            "  üìÅ UCSF-PDGM-0005_nifti/\n",
            "  üìÅ UCSF-PDGM-0007_nifti/\n",
            "  üìÅ UCSF-PDGM-0008_nifti/\n",
            "  üìÅ UCSF-PDGM-0009_nifti/\n",
            "  üìÅ UCSF-PDGM-0010_nifti/\n",
            "  üìÅ UCSF-PDGM-0011_nifti/\n",
            "  üìÅ UCSF-PDGM-0012_nifti/\n",
            "  üìÅ UCSF-PDGM-0013_nifti/\n",
            "  üìÅ UCSF-PDGM-0014_nifti/\n",
            "  üìÅ UCSF-PDGM-0015_nifti/\n",
            "  üìÅ UCSF-PDGM-0016_nifti/\n",
            "  üìÅ UCSF-PDGM-0017_nifti/\n",
            "  üìÅ UCSF-PDGM-0018_nifti/\n",
            "  üìÅ UCSF-PDGM-0019_nifti/\n",
            "  üìÅ UCSF-PDGM-0020_nifti/\n",
            "  üìÅ UCSF-PDGM-0021_nifti/\n",
            "  üìÅ UCSF-PDGM-0022_nifti/\n",
            "  üìÅ UCSF-PDGM-0023_nifti/\n",
            "  üìÅ UCSF-PDGM-0024_nifti/\n",
            "  üìÅ UCSF-PDGM-0025_nifti/\n",
            "  üìÅ UCSF-PDGM-0026_nifti/\n",
            "  üìÅ UCSF-PDGM-0027_nifti/\n",
            "  üìÅ UCSF-PDGM-0029_nifti/\n",
            "  üìÅ UCSF-PDGM-0030_nifti/\n",
            "  üìÅ UCSF-PDGM-0031_nifti/\n",
            "  üìÅ UCSF-PDGM-0032_nifti/\n",
            "  üìÅ UCSF-PDGM-0033_nifti/\n",
            "  üìÅ UCSF-PDGM-0034_nifti/\n",
            "  üìÅ UCSF-PDGM-0035_nifti/\n",
            "  üìÅ UCSF-PDGM-0036_nifti/\n",
            "  üìÅ UCSF-PDGM-0037_nifti/\n",
            "  üìÅ UCSF-PDGM-0038_nifti/\n",
            "  üìÅ UCSF-PDGM-0039_nifti/\n",
            "  üìÅ UCSF-PDGM-0040_nifti/\n",
            "  üìÅ UCSF-PDGM-0041_nifti/\n",
            "  üìÅ UCSF-PDGM-0042_nifti/\n",
            "  üìÅ UCSF-PDGM-0043_nifti/\n",
            "  üìÅ UCSF-PDGM-0044_nifti/\n",
            "  üìÅ UCSF-PDGM-0045_nifti/\n",
            "  üìÅ UCSF-PDGM-0046_nifti/\n",
            "  üìÅ UCSF-PDGM-0047_nifti/\n",
            "  üìÅ UCSF-PDGM-0048_nifti/\n",
            "  üìÅ UCSF-PDGM-0049_nifti/\n",
            "  üìÅ UCSF-PDGM-0050_nifti/\n",
            "  üìÅ UCSF-PDGM-0053_nifti/\n",
            "  üìÅ UCSF-PDGM-0055_nifti/\n",
            "  üìÅ UCSF-PDGM-0056_nifti/\n",
            "  üìÅ UCSF-PDGM-0057_nifti/\n",
            "  üìÅ UCSF-PDGM-0058_nifti/\n",
            "  üìÅ UCSF-PDGM-0059_nifti/\n",
            "  üìÅ UCSF-PDGM-0061_nifti/\n",
            "  üìÅ UCSF-PDGM-0063_nifti/\n",
            "  üìÅ UCSF-PDGM-0064_nifti/\n",
            "  üìÅ UCSF-PDGM-0065_nifti/\n",
            "  üìÅ UCSF-PDGM-0066_nifti/\n",
            "  üìÅ UCSF-PDGM-0067_nifti/\n",
            "  üìÅ UCSF-PDGM-0068_nifti/\n",
            "  üìÅ UCSF-PDGM-0069_nifti/\n",
            "  üìÅ UCSF-PDGM-0070_nifti/\n",
            "  üìÅ UCSF-PDGM-0071_nifti/\n",
            "  üìÅ UCSF-PDGM-0072_nifti/\n",
            "  üìÅ UCSF-PDGM-0073_nifti/\n",
            "  üìÅ UCSF-PDGM-0074_nifti/\n",
            "  üìÅ UCSF-PDGM-0075_nifti/\n",
            "  üìÅ UCSF-PDGM-0076_nifti/\n",
            "  üìÅ UCSF-PDGM-0077_nifti/\n",
            "  üìÅ UCSF-PDGM-0078_nifti/\n",
            "  üìÅ UCSF-PDGM-0079_nifti/\n",
            "  üìÅ UCSF-PDGM-0080_nifti/\n",
            "  üìÅ UCSF-PDGM-0082_nifti/\n",
            "  üìÅ UCSF-PDGM-0083_nifti/\n",
            "  üìÅ UCSF-PDGM-0084_nifti/\n",
            "  üìÅ UCSF-PDGM-0085_nifti/\n",
            "  üìÅ UCSF-PDGM-0086_nifti/\n",
            "  üìÅ UCSF-PDGM-0087_nifti/\n",
            "  üìÅ UCSF-PDGM-0088_nifti/\n",
            "  üìÅ UCSF-PDGM-0089_nifti/\n",
            "  üìÅ UCSF-PDGM-0090_nifti/\n",
            "  üìÅ UCSF-PDGM-0091_nifti/\n",
            "  üìÅ UCSF-PDGM-0092_nifti/\n",
            "  üìÅ UCSF-PDGM-0093_nifti/\n",
            "  üìÅ UCSF-PDGM-0094_nifti/\n",
            "  üìÅ UCSF-PDGM-0095_nifti/\n",
            "  üìÅ UCSF-PDGM-0096_nifti/\n",
            "  üìÅ UCSF-PDGM-0097_nifti/\n",
            "  üìÅ UCSF-PDGM-0099_nifti/\n",
            "  üìÅ UCSF-PDGM-0101_nifti/\n",
            "  üìÅ UCSF-PDGM-0102_nifti/\n",
            "  üìÅ UCSF-PDGM-0103_nifti/\n",
            "  üìÅ UCSF-PDGM-0104_nifti/\n",
            "  üìÅ UCSF-PDGM-0105_nifti/\n",
            "  üìÅ UCSF-PDGM-0106_nifti/\n",
            "  üìÅ UCSF-PDGM-0107_nifti/\n",
            "  üìÅ UCSF-PDGM-0108_nifti/\n",
            "  üìÅ UCSF-PDGM-0109_nifti/\n",
            "  üìÅ UCSF-PDGM-0111_nifti/\n",
            "  üìÅ UCSF-PDGM-0112_nifti/\n",
            "  üìÅ UCSF-PDGM-0113_nifti/\n",
            "  üìÅ UCSF-PDGM-0114_nifti/\n",
            "  üìÅ UCSF-PDGM-0115_nifti/\n",
            "  üìÅ UCSF-PDGM-0116_nifti/\n",
            "  üìÅ UCSF-PDGM-0118_nifti/\n",
            "  üìÅ UCSF-PDGM-0119_nifti/\n",
            "  üìÅ UCSF-PDGM-0121_nifti/\n",
            "  üìÅ UCSF-PDGM-0122_nifti/\n",
            "  üìÅ UCSF-PDGM-0123_nifti/\n",
            "  üìÅ UCSF-PDGM-0124_nifti/\n",
            "  üìÅ UCSF-PDGM-0126_nifti/\n",
            "  üìÅ UCSF-PDGM-0127_nifti/\n",
            "  üìÅ UCSF-PDGM-0128_nifti/\n",
            "  üìÅ UCSF-PDGM-0129_nifti/\n",
            "  üìÅ UCSF-PDGM-0130_nifti/\n",
            "  üìÅ UCSF-PDGM-0131_nifti/\n",
            "  üìÅ UCSF-PDGM-0132_nifti/\n",
            "  üìÅ UCSF-PDGM-0133_nifti/\n",
            "  üìÅ UCSF-PDGM-0134_nifti/\n",
            "  üìÅ UCSF-PDGM-0135_nifti/\n",
            "  üìÅ UCSF-PDGM-0136_nifti/\n",
            "  üìÅ UCSF-PDGM-0137_nifti/\n",
            "  üìÅ UCSF-PDGM-0139_nifti/\n",
            "  üìÅ UCSF-PDGM-0140_nifti/\n",
            "  üìÅ UCSF-PDGM-0141_nifti/\n",
            "  üìÅ UCSF-PDGM-0142_nifti/\n",
            "  üìÅ UCSF-PDGM-0143_nifti/\n",
            "  üìÅ UCSF-PDGM-0144_nifti/\n",
            "  üìÅ UCSF-PDGM-0145_nifti/\n",
            "  üìÅ UCSF-PDGM-0146_nifti/\n",
            "  üìÅ UCSF-PDGM-0147_nifti/\n",
            "  üìÅ UCSF-PDGM-0148_nifti/\n",
            "  üìÅ UCSF-PDGM-0149_nifti/\n",
            "  üìÅ UCSF-PDGM-0150_nifti/\n",
            "  üìÅ UCSF-PDGM-0151_nifti/\n",
            "  üìÅ UCSF-PDGM-0152_nifti/\n",
            "  üìÅ UCSF-PDGM-0153_nifti/\n",
            "  üìÅ UCSF-PDGM-0154_nifti/\n",
            "  üìÅ UCSF-PDGM-0155_nifti/\n",
            "  üìÅ UCSF-PDGM-0156_nifti/\n",
            "  üìÅ UCSF-PDGM-0157_nifti/\n",
            "  üìÅ UCSF-PDGM-0158_nifti/\n",
            "  üìÅ UCSF-PDGM-0159_nifti/\n",
            "  üìÅ UCSF-PDGM-0160_nifti/\n",
            "  üìÅ UCSF-PDGM-0161_nifti/\n",
            "  üìÅ UCSF-PDGM-0162_nifti/\n",
            "  üìÅ UCSF-PDGM-0163_nifti/\n",
            "  üìÅ UCSF-PDGM-0164_nifti/\n",
            "  üìÅ UCSF-PDGM-0165_nifti/\n",
            "  üìÅ UCSF-PDGM-0166_nifti/\n",
            "  üìÅ UCSF-PDGM-0167_nifti/\n",
            "  üìÅ UCSF-PDGM-0168_nifti/\n",
            "  üìÅ UCSF-PDGM-0169_nifti/\n",
            "  üìÅ UCSF-PDGM-0170_nifti/\n",
            "  üìÅ UCSF-PDGM-0172_nifti/\n",
            "  üìÅ UCSF-PDGM-0173_nifti/\n",
            "  üìÅ UCSF-PDGM-0174_nifti/\n",
            "  üìÅ UCSF-PDGM-0176_nifti/\n",
            "  üìÅ UCSF-PDGM-0178_nifti/\n",
            "  üìÅ UCSF-PDGM-0179_nifti/\n",
            "  üìÅ UCSF-PDGM-0180_nifti/\n",
            "  üìÅ UCSF-PDGM-0182_nifti/\n",
            "  üìÅ UCSF-PDGM-0183_nifti/\n",
            "  üìÅ UCSF-PDGM-0184_nifti/\n",
            "  üìÅ UCSF-PDGM-0185_nifti/\n",
            "  üìÅ UCSF-PDGM-0186_nifti/\n",
            "  üìÅ UCSF-PDGM-0187_nifti/\n",
            "  üìÅ UCSF-PDGM-0188_nifti/\n",
            "  üìÅ UCSF-PDGM-0189_nifti/\n",
            "  üìÅ UCSF-PDGM-0190_nifti/\n",
            "  üìÅ UCSF-PDGM-0191_nifti/\n",
            "  üìÅ UCSF-PDGM-0193_nifti/\n",
            "  üìÅ UCSF-PDGM-0194_nifti/\n",
            "  üìÅ UCSF-PDGM-0195_nifti/\n",
            "  üìÅ UCSF-PDGM-0196_nifti/\n",
            "  üìÅ UCSF-PDGM-0197_nifti/\n",
            "  üìÅ UCSF-PDGM-0198_nifti/\n",
            "  üìÅ UCSF-PDGM-0200_nifti/\n",
            "  üìÅ UCSF-PDGM-0201_nifti/\n",
            "  üìÅ UCSF-PDGM-0202_nifti/\n",
            "  üìÅ UCSF-PDGM-0203_nifti/\n",
            "  üìÅ UCSF-PDGM-0204_nifti/\n",
            "  üìÅ UCSF-PDGM-0205_nifti/\n",
            "  üìÅ UCSF-PDGM-0206_nifti/\n",
            "  üìÅ UCSF-PDGM-0207_nifti/\n",
            "  üìÅ UCSF-PDGM-0208_nifti/\n",
            "  üìÅ UCSF-PDGM-0209_nifti/\n",
            "  üìÅ UCSF-PDGM-0210_nifti/\n",
            "  üìÅ UCSF-PDGM-0212_nifti/\n",
            "  üìÅ UCSF-PDGM-0213_nifti/\n",
            "  üìÅ UCSF-PDGM-0214_nifti/\n",
            "  üìÅ UCSF-PDGM-0215_nifti/\n",
            "  üìÅ UCSF-PDGM-0223_nifti/\n",
            "  üìÅ UCSF-PDGM-0225_nifti/\n",
            "  üìÅ UCSF-PDGM-0227_nifti/\n",
            "  üìÅ UCSF-PDGM-0228_nifti/\n",
            "  üìÅ UCSF-PDGM-0229_nifti/\n",
            "  üìÅ UCSF-PDGM-0231_nifti/\n",
            "  üìÅ UCSF-PDGM-0232_nifti/\n",
            "  üìÅ UCSF-PDGM-0233_nifti/\n",
            "  üìÅ UCSF-PDGM-0234_nifti/\n",
            "  üìÅ UCSF-PDGM-0235_nifti/\n",
            "  üìÅ UCSF-PDGM-0236_nifti/\n",
            "  üìÅ UCSF-PDGM-0237_nifti/\n",
            "  üìÅ UCSF-PDGM-0238_nifti/\n",
            "  üìÅ UCSF-PDGM-0239_nifti/\n",
            "  üìÅ UCSF-PDGM-0240_nifti/\n",
            "  üìÅ UCSF-PDGM-0241_nifti/\n",
            "  üìÅ UCSF-PDGM-0242_nifti/\n",
            "  üìÅ UCSF-PDGM-0243_nifti/\n",
            "  üìÅ UCSF-PDGM-0244_nifti/\n",
            "  üìÅ UCSF-PDGM-0245_nifti/\n",
            "  üìÅ UCSF-PDGM-0246_nifti/\n",
            "  üìÅ UCSF-PDGM-0247_nifti/\n",
            "  üìÅ UCSF-PDGM-0248_nifti/\n",
            "  üìÅ UCSF-PDGM-0249_nifti/\n",
            "  üìÅ UCSF-PDGM-0250_nifti/\n",
            "  üìÅ UCSF-PDGM-0251_nifti/\n",
            "  üìÅ UCSF-PDGM-0252_nifti/\n",
            "  üìÅ UCSF-PDGM-0253_nifti/\n",
            "  üìÅ UCSF-PDGM-0254_nifti/\n",
            "  üìÅ UCSF-PDGM-0255_nifti/\n",
            "  üìÅ UCSF-PDGM-0256_nifti/\n",
            "  üìÅ UCSF-PDGM-0257_nifti/\n",
            "  üìÅ UCSF-PDGM-0258_nifti/\n",
            "  üìÅ UCSF-PDGM-0259_nifti/\n",
            "  üìÅ UCSF-PDGM-0260_nifti/\n",
            "  üìÅ UCSF-PDGM-0261_nifti/\n",
            "  üìÅ UCSF-PDGM-0262_nifti/\n",
            "  üìÅ UCSF-PDGM-0263_nifti/\n",
            "  üìÅ UCSF-PDGM-0264_nifti/\n",
            "  üìÅ UCSF-PDGM-0265_nifti/\n",
            "  üìÅ UCSF-PDGM-0266_nifti/\n",
            "  üìÅ UCSF-PDGM-0267_nifti/\n",
            "  üìÅ UCSF-PDGM-0268_nifti/\n",
            "  üìÅ UCSF-PDGM-0269_nifti/\n",
            "  üìÅ UCSF-PDGM-0270_nifti/\n",
            "  üìÅ UCSF-PDGM-0272_nifti/\n",
            "  üìÅ UCSF-PDGM-0273_nifti/\n",
            "  üìÅ UCSF-PDGM-0274_nifti/\n",
            "  üìÅ UCSF-PDGM-0275_nifti/\n",
            "  üìÅ UCSF-PDGM-0276_nifti/\n",
            "  üìÅ UCSF-PDGM-0277_nifti/\n",
            "  üìÅ UCSF-PDGM-0279_nifti/\n",
            "  üìÅ UCSF-PDGM-0280_nifti/\n",
            "  üìÅ UCSF-PDGM-0281_nifti/\n",
            "  üìÅ UCSF-PDGM-0282_nifti/\n",
            "  üìÅ UCSF-PDGM-0283_nifti/\n",
            "  üìÅ UCSF-PDGM-0284_nifti/\n",
            "  üìÅ UCSF-PDGM-0285_nifti/\n",
            "  üìÅ UCSF-PDGM-0286_nifti/\n",
            "  üìÅ UCSF-PDGM-0287_nifti/\n",
            "  üìÅ UCSF-PDGM-0288_nifti/\n",
            "  üìÅ UCSF-PDGM-0290_nifti/\n",
            "  üìÅ UCSF-PDGM-0291_nifti/\n",
            "  üìÅ UCSF-PDGM-0295_nifti/\n",
            "  üìÅ UCSF-PDGM-0296_nifti/\n",
            "  üìÅ UCSF-PDGM-0297_nifti/\n",
            "  üìÅ UCSF-PDGM-0298_nifti/\n",
            "  üìÅ UCSF-PDGM-0300_nifti/\n",
            "  üìÅ UCSF-PDGM-0302_nifti/\n",
            "  üìÅ UCSF-PDGM-0303_nifti/\n",
            "  üìÅ UCSF-PDGM-0304_nifti/\n",
            "  üìÅ UCSF-PDGM-0305_nifti/\n",
            "  üìÅ UCSF-PDGM-0306_nifti/\n",
            "  üìÅ UCSF-PDGM-0307_nifti/\n",
            "  üìÅ UCSF-PDGM-0308_nifti/\n",
            "  üìÅ UCSF-PDGM-0309_nifti/\n",
            "  üìÅ UCSF-PDGM-0310_nifti/\n",
            "  üìÅ UCSF-PDGM-0311_nifti/\n",
            "  üìÅ UCSF-PDGM-0312_nifti/\n",
            "  üìÅ UCSF-PDGM-0313_nifti/\n",
            "  üìÅ UCSF-PDGM-0314_nifti/\n",
            "  üìÅ UCSF-PDGM-0316_nifti/\n",
            "  üìÅ UCSF-PDGM-0317_nifti/\n",
            "  üìÅ UCSF-PDGM-0318_nifti/\n",
            "  üìÅ UCSF-PDGM-0319_nifti/\n",
            "  üìÅ UCSF-PDGM-0320_nifti/\n",
            "  üìÅ UCSF-PDGM-0321_nifti/\n",
            "  üìÅ UCSF-PDGM-0323_nifti/\n",
            "  üìÅ UCSF-PDGM-0324_nifti/\n",
            "  üìÅ UCSF-PDGM-0325_nifti/\n",
            "  üìÅ UCSF-PDGM-0326_nifti/\n",
            "  üìÅ UCSF-PDGM-0327_nifti/\n",
            "  üìÅ UCSF-PDGM-0328_nifti/\n",
            "  üìÅ UCSF-PDGM-0329_nifti/\n",
            "  üìÅ UCSF-PDGM-0330_nifti/\n",
            "  üìÅ UCSF-PDGM-0331_nifti/\n",
            "  üìÅ UCSF-PDGM-0332_nifti/\n",
            "  üìÅ UCSF-PDGM-0333_nifti/\n",
            "  üìÅ UCSF-PDGM-0334_nifti/\n",
            "  üìÅ UCSF-PDGM-0335_nifti/\n",
            "  üìÅ UCSF-PDGM-0336_nifti/\n",
            "  üìÅ UCSF-PDGM-0337_nifti/\n",
            "  üìÅ UCSF-PDGM-0338_nifti/\n",
            "  üìÅ UCSF-PDGM-0339_nifti/\n",
            "  üìÅ UCSF-PDGM-0340_nifti/\n",
            "  üìÅ UCSF-PDGM-0341_nifti/\n",
            "  üìÅ UCSF-PDGM-0342_nifti/\n",
            "  üìÅ UCSF-PDGM-0343_nifti/\n",
            "  üìÅ UCSF-PDGM-0344_nifti/\n",
            "  üìÅ UCSF-PDGM-0345_nifti/\n",
            "  üìÅ UCSF-PDGM-0346_nifti/\n",
            "  üìÅ UCSF-PDGM-0347_nifti/\n",
            "  üìÅ UCSF-PDGM-0348_nifti/\n",
            "  üìÅ UCSF-PDGM-0349_nifti/\n",
            "  üìÅ UCSF-PDGM-0350_nifti/\n",
            "  üìÅ UCSF-PDGM-0351_nifti/\n",
            "  üìÅ UCSF-PDGM-0352_nifti/\n",
            "  üìÅ UCSF-PDGM-0353_nifti/\n",
            "  üìÅ UCSF-PDGM-0354_nifti/\n",
            "  üìÅ UCSF-PDGM-0355_nifti/\n",
            "  üìÅ UCSF-PDGM-0356_nifti/\n",
            "  üìÅ UCSF-PDGM-0357_nifti/\n",
            "  üìÅ UCSF-PDGM-0358_nifti/\n",
            "  üìÅ UCSF-PDGM-0359_nifti/\n",
            "  üìÅ UCSF-PDGM-0360_nifti/\n",
            "  üìÅ UCSF-PDGM-0361_nifti/\n",
            "  üìÅ UCSF-PDGM-0362_nifti/\n",
            "  üìÅ UCSF-PDGM-0363_nifti/\n",
            "  üìÅ UCSF-PDGM-0364_nifti/\n",
            "  üìÅ UCSF-PDGM-0365_nifti/\n",
            "  üìÅ UCSF-PDGM-0366_nifti/\n",
            "  üìÅ UCSF-PDGM-0367_nifti/\n",
            "  üìÅ UCSF-PDGM-0368_nifti/\n",
            "  üìÅ UCSF-PDGM-0369_nifti/\n",
            "  üìÅ UCSF-PDGM-0370_nifti/\n",
            "  üìÅ UCSF-PDGM-0371_nifti/\n",
            "  üìÅ UCSF-PDGM-0372_nifti/\n",
            "  üìÅ UCSF-PDGM-0373_nifti/\n",
            "  üìÅ UCSF-PDGM-0374_nifti/\n",
            "  üìÅ UCSF-PDGM-0375_nifti/\n",
            "  üìÅ UCSF-PDGM-0376_nifti/\n",
            "  üìÅ UCSF-PDGM-0377_nifti/\n",
            "  üìÅ UCSF-PDGM-0378_nifti/\n",
            "  üìÅ UCSF-PDGM-0379_nifti/\n",
            "  üìÅ UCSF-PDGM-0380_nifti/\n",
            "  üìÅ UCSF-PDGM-0381_nifti/\n",
            "  üìÅ UCSF-PDGM-0382_nifti/\n",
            "  üìÅ UCSF-PDGM-0383_nifti/\n",
            "  üìÅ UCSF-PDGM-0384_nifti/\n",
            "  üìÅ UCSF-PDGM-0385_nifti/\n",
            "  üìÅ UCSF-PDGM-0386_nifti/\n",
            "  üìÅ UCSF-PDGM-0387_nifti/\n",
            "  üìÅ UCSF-PDGM-0388_nifti/\n",
            "  üìÅ UCSF-PDGM-0389_nifti/\n",
            "  üìÅ UCSF-PDGM-0390_nifti/\n",
            "  üìÅ UCSF-PDGM-0391_FU016d_nifti/\n",
            "  üìÅ UCSF-PDGM-0391_nifti/\n",
            "  üìÅ UCSF-PDGM-0392_nifti/\n",
            "  üìÅ UCSF-PDGM-0393_nifti/\n",
            "  üìÅ UCSF-PDGM-0394_nifti/\n",
            "  üìÅ UCSF-PDGM-0395_nifti/\n",
            "  üìÅ UCSF-PDGM-0396_FU175d_nifti/\n",
            "  üìÅ UCSF-PDGM-0396_nifti/\n",
            "  üìÅ UCSF-PDGM-0397_nifti/\n",
            "  üìÅ UCSF-PDGM-0398_nifti/\n",
            "  üìÅ UCSF-PDGM-0399_nifti/\n",
            "  üìÅ UCSF-PDGM-0400_nifti/\n",
            "  üìÅ UCSF-PDGM-0401_nifti/\n",
            "  üìÅ UCSF-PDGM-0402_nifti/\n",
            "  üìÅ UCSF-PDGM-0403_nifti/\n",
            "  üìÅ UCSF-PDGM-0404_nifti/\n",
            "  üìÅ UCSF-PDGM-0405_nifti/\n",
            "  üìÅ UCSF-PDGM-0406_nifti/\n",
            "  üìÅ UCSF-PDGM-0407_nifti/\n",
            "  üìÅ UCSF-PDGM-0409_FU001d_nifti/\n",
            "  üìÅ UCSF-PDGM-0409_nifti/\n",
            "  üìÅ UCSF-PDGM-0410_nifti/\n",
            "  üìÅ UCSF-PDGM-0411_nifti/\n",
            "  üìÅ UCSF-PDGM-0412_nifti/\n",
            "  üìÅ UCSF-PDGM-0413_nifti/\n",
            "  üìÅ UCSF-PDGM-0414_nifti/\n",
            "  üìÅ UCSF-PDGM-0415_nifti/\n",
            "  üìÅ UCSF-PDGM-0416_nifti/\n",
            "  üìÅ UCSF-PDGM-0417_nifti/\n",
            "  üìÅ UCSF-PDGM-0418_nifti/\n",
            "  üìÅ UCSF-PDGM-0419_nifti/\n",
            "  üìÅ UCSF-PDGM-0420_nifti/\n",
            "  üìÅ UCSF-PDGM-0421_nifti/\n",
            "  üìÅ UCSF-PDGM-0422_nifti/\n",
            "  üìÅ UCSF-PDGM-0423_nifti/\n",
            "  üìÅ UCSF-PDGM-0424_nifti/\n",
            "  üìÅ UCSF-PDGM-0425_nifti/\n",
            "  üìÅ UCSF-PDGM-0426_nifti/\n",
            "  üìÅ UCSF-PDGM-0427_nifti/\n",
            "  üìÅ UCSF-PDGM-0428_nifti/\n",
            "  üìÅ UCSF-PDGM-0429_FU003d_nifti/\n",
            "  üìÅ UCSF-PDGM-0429_nifti/\n",
            "  üìÅ UCSF-PDGM-0430_nifti/\n",
            "  üìÅ UCSF-PDGM-0431_FU001d_nifti/\n",
            "  üìÅ UCSF-PDGM-0431_nifti/\n",
            "  üìÅ UCSF-PDGM-0432_nifti/\n",
            "  üìÅ UCSF-PDGM-0433_FU007d_nifti/\n",
            "  üìÅ UCSF-PDGM-0433_nifti/\n",
            "  üìÅ UCSF-PDGM-0434_nifti/\n",
            "  üìÅ UCSF-PDGM-0435_nifti/\n",
            "  üìÅ UCSF-PDGM-0436_nifti/\n",
            "  üìÅ UCSF-PDGM-0437_nifti/\n",
            "  üìÅ UCSF-PDGM-0438_nifti/\n",
            "  üìÅ UCSF-PDGM-0439_nifti/\n",
            "  üìÅ UCSF-PDGM-0440_nifti/\n",
            "  üìÅ UCSF-PDGM-0441_nifti/\n",
            "  üìÅ UCSF-PDGM-0442_nifti/\n",
            "  üìÅ UCSF-PDGM-0443_nifti/\n",
            "  üìÅ UCSF-PDGM-0444_nifti/\n",
            "  üìÅ UCSF-PDGM-0445_nifti/\n",
            "  üìÅ UCSF-PDGM-0446_nifti/\n",
            "  üìÅ UCSF-PDGM-0447_nifti/\n",
            "  üìÅ UCSF-PDGM-0448_nifti/\n",
            "  üìÅ UCSF-PDGM-0449_nifti/\n",
            "  üìÅ UCSF-PDGM-0450_nifti/\n",
            "  üìÅ UCSF-PDGM-0451_nifti/\n",
            "  üìÅ UCSF-PDGM-0452_nifti/\n",
            "  üìÅ UCSF-PDGM-0453_nifti/\n",
            "  üìÅ UCSF-PDGM-0454_nifti/\n",
            "  üìÅ UCSF-PDGM-0455_nifti/\n",
            "  üìÅ UCSF-PDGM-0456_nifti/\n",
            "  üìÅ UCSF-PDGM-0457_nifti/\n",
            "  üìÅ UCSF-PDGM-0458_nifti/\n",
            "  üìÅ UCSF-PDGM-0459_nifti/\n",
            "  üìÅ UCSF-PDGM-0460_nifti/\n",
            "  üìÅ UCSF-PDGM-0461_nifti/\n",
            "  üìÅ UCSF-PDGM-0462_nifti/\n",
            "  üìÅ UCSF-PDGM-0463_nifti/\n",
            "  üìÅ UCSF-PDGM-0464_nifti/\n",
            "  üìÅ UCSF-PDGM-0465_nifti/\n",
            "  üìÅ UCSF-PDGM-0466_nifti/\n",
            "  üìÅ UCSF-PDGM-0467_nifti/\n",
            "  üìÅ UCSF-PDGM-0468_nifti/\n",
            "  üìÅ UCSF-PDGM-0469_nifti/\n",
            "  üìÅ UCSF-PDGM-0470_nifti/\n",
            "  üìÅ UCSF-PDGM-0471_nifti/\n",
            "  üìÅ UCSF-PDGM-0472_nifti/\n",
            "  üìÅ UCSF-PDGM-0473_nifti/\n",
            "  üìÅ UCSF-PDGM-0474_nifti/\n",
            "  üìÅ UCSF-PDGM-0475_nifti/\n",
            "  üìÅ UCSF-PDGM-0476_nifti/\n",
            "  üìÅ UCSF-PDGM-0477_nifti/\n",
            "  üìÅ UCSF-PDGM-0478_nifti/\n",
            "  üìÅ UCSF-PDGM-0479_nifti/\n",
            "  üìÅ UCSF-PDGM-0480_nifti/\n",
            "  üìÅ UCSF-PDGM-0481_nifti/\n",
            "  üìÅ UCSF-PDGM-0482_nifti/\n",
            "  üìÅ UCSF-PDGM-0483_nifti/\n",
            "  üìÅ UCSF-PDGM-0484_nifti/\n",
            "  üìÅ UCSF-PDGM-0485_nifti/\n",
            "  üìÅ UCSF-PDGM-0486_nifti/\n",
            "  üìÅ UCSF-PDGM-0487_nifti/\n",
            "  üìÅ UCSF-PDGM-0488_nifti/\n",
            "  üìÅ UCSF-PDGM-0489_nifti/\n",
            "  üìÅ UCSF-PDGM-0490_nifti/\n",
            "  üìÅ UCSF-PDGM-0491_nifti/\n",
            "  üìÅ UCSF-PDGM-0492_nifti/\n",
            "  üìÅ UCSF-PDGM-0493_nifti/\n",
            "  üìÅ UCSF-PDGM-0494_nifti/\n",
            "  üìÅ UCSF-PDGM-0495_nifti/\n",
            "  üìÅ UCSF-PDGM-0496_nifti/\n",
            "  üìÅ UCSF-PDGM-0497_nifti/\n",
            "  üìÅ UCSF-PDGM-0498_nifti/\n",
            "  üìÅ UCSF-PDGM-0499_nifti/\n",
            "  üìÅ UCSF-PDGM-0500_nifti/\n",
            "  üìÅ UCSF-PDGM-0501_nifti/\n",
            "  üìÅ UCSF-PDGM-0502_nifti/\n",
            "  üìÅ UCSF-PDGM-0503_nifti/\n",
            "  üìÅ UCSF-PDGM-0504_nifti/\n",
            "  üìÅ UCSF-PDGM-0505_nifti/\n",
            "  üìÅ UCSF-PDGM-0506_nifti/\n",
            "  üìÅ UCSF-PDGM-0507_nifti/\n",
            "  üìÅ UCSF-PDGM-0508_nifti/\n",
            "  üìÅ UCSF-PDGM-0509_nifti/\n",
            "  üìÅ UCSF-PDGM-0510_nifti/\n",
            "  üìÅ UCSF-PDGM-0511_nifti/\n",
            "  üìÅ UCSF-PDGM-0512_nifti/\n",
            "  üìÅ UCSF-PDGM-0513_nifti/\n",
            "  üìÅ UCSF-PDGM-0514_nifti/\n",
            "  üìÅ UCSF-PDGM-0515_nifti/\n",
            "  üìÅ UCSF-PDGM-0516_nifti/\n",
            "  üìÅ UCSF-PDGM-0517_nifti/\n",
            "  üìÅ UCSF-PDGM-0518_nifti/\n",
            "  üìÅ UCSF-PDGM-0519_nifti/\n",
            "  üìÅ UCSF-PDGM-0520_nifti/\n",
            "  üìÅ UCSF-PDGM-0521_nifti/\n",
            "  üìÅ UCSF-PDGM-0522_nifti/\n",
            "  üìÅ UCSF-PDGM-0523_nifti/\n",
            "  üìÅ UCSF-PDGM-0524_nifti/\n",
            "  üìÅ UCSF-PDGM-0525_nifti/\n",
            "  üìÅ UCSF-PDGM-0526_nifti/\n",
            "  üìÅ UCSF-PDGM-0527_nifti/\n",
            "  üìÅ UCSF-PDGM-0528_nifti/\n",
            "  üìÅ UCSF-PDGM-0529_nifti/\n",
            "  üìÅ UCSF-PDGM-0530_nifti/\n",
            "  üìÅ UCSF-PDGM-0531_nifti/\n",
            "  üìÅ UCSF-PDGM-0532_nifti/\n",
            "  üìÅ UCSF-PDGM-0533_nifti/\n",
            "  üìÅ UCSF-PDGM-0534_nifti/\n",
            "  üìÅ UCSF-PDGM-0535_nifti/\n",
            "  üìÅ UCSF-PDGM-0536_nifti/\n",
            "  üìÅ UCSF-PDGM-0537_nifti/\n",
            "  üìÅ UCSF-PDGM-0538_nifti/\n",
            "  üìÅ UCSF-PDGM-0539_nifti/\n",
            "  üìÅ UCSF-PDGM-0540_nifti/\n",
            "  üìÅ UCSF-PDGM-0541_nifti/\n",
            "Total patient folders: 501\n",
            "\n",
            "First 5 patients:\n",
            "  üìÅ UCSF-PDGM-0004_nifti\n",
            "  üìÅ UCSF-PDGM-0005_nifti\n",
            "  üìÅ UCSF-PDGM-0007_nifti\n",
            "  üìÅ UCSF-PDGM-0008_nifti\n",
            "  üìÅ UCSF-PDGM-0009_nifti\n",
            "\n",
            "üìÇ Scans in UCSF-PDGM-0004_nifti:\n",
            "Total scans: 7\n",
            "\n",
            "1. UCSF-PDGM-0004_ADC.nii.gz\n",
            "2. UCSF-PDGM-0004_FLAIR.nii.gz\n",
            "3. UCSF-PDGM-0004_T1.nii.gz\n",
            "4. UCSF-PDGM-0004_T1c.nii.gz\n",
            "5. UCSF-PDGM-0004_T2.nii.gz\n",
            "6. UCSF-PDGM-0004_brain_segmentation.nii.gz\n",
            "7. UCSF-PDGM-0004_tumor_segmentation.nii.gz\n",
            "\n",
            "üìÑ Looking for label files in base directory:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient_folders = sorted([f for f in os.listdir(base_path)\n",
        "                         if os.path.isdir(os.path.join(base_path, f))])\n",
        "\n",
        "tumor_count = 0\n",
        "no_tumor_count = 0\n",
        "error_count = 0\n",
        "\n",
        "print(f\"Checking all {len(patient_folders)} patients...\\n\")\n",
        "\n",
        "for patient in patient_folders:\n",
        "    tumor_seg_path = os.path.join(base_path, patient,\n",
        "                                   f\"{patient.replace('_nifti', '')}_tumor_segmentation.nii.gz\")\n",
        "\n",
        "    try:\n",
        "        img = nib.load(tumor_seg_path).get_fdata()\n",
        "        if np.any(img > 0):\n",
        "            tumor_count += 1\n",
        "        else:\n",
        "            no_tumor_count += 1\n",
        "            print(f\"NO TUMOR: {patient}\")  # Print patients without tumor\n",
        "    except:\n",
        "        error_count += 1\n",
        "        print(f\"ERROR loading: {patient}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"SUMMARY:\")\n",
        "print(f\"  Total patients: {len(patient_folders)}\")\n",
        "print(f\"  Has tumor: {tumor_count}\")\n",
        "print(f\"  No tumor: {no_tumor_count}\")\n",
        "print(f\"  Errors: {error_count}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8mZVNRSnu5q",
        "outputId": "912a653e-5035-4531-fc00-f989366dc53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking all 501 patients...\n",
            "\n",
            "\n",
            "==================================================\n",
            "SUMMARY:\n",
            "  Total patients: 501\n",
            "  Has tumor: 501\n",
            "  No tumor: 0\n",
            "  Errors: 0\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your CSV file\n",
        "csv_path = \"https://raw.githubusercontent.com/Erdos-Projects/fall-2025-brain-tumor-diagnosis/refs/heads/main/Clinical-data/UCSF-PDGM-metadata_v5.csv\"\n",
        "\n",
        "# Read the CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display basic info\n",
        "print(f\"CSV loaded successfully!\")\n",
        "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\\n\")\n",
        "\n",
        "# Show column names\n",
        "print(\"Column names:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i}. {col}\")\n",
        "\n",
        "# Show first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check if there's a patient ID column that matches our folder names\n",
        "print(\"\\nChecking for patient ID column...\")\n",
        "potential_id_cols = [col for col in df.columns if 'id' in col.lower() or 'patient' in col.lower() or 'case' in col.lower()]\n",
        "if potential_id_cols:\n",
        "    print(f\"Potential ID columns: {potential_id_cols}\")\n",
        "    print(f\"\\nSample values from '{potential_id_cols[0]}':\")\n",
        "    print(df[potential_id_cols[0]].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyDirrjCn1L8",
        "outputId": "c8fefc18-985a-4ff5-a77e-ab8d926e6714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV loaded successfully!\n",
            "Shape: 501 rows √ó 16 columns\n",
            "\n",
            "Column names:\n",
            "  1. ID\n",
            "  2. Sex\n",
            "  3. Age at MRI\n",
            "  4. WHO CNS Grade\n",
            "  5. Final pathologic diagnosis (WHO 2021)\n",
            "  6. MGMT status\n",
            "  7. MGMT index\n",
            "  8. 1p/19q\n",
            "  9. IDH\n",
            "  10. 1-dead 0-alive\n",
            "  11. OS\n",
            "  12. EOR\n",
            "  13. Biopsy prior to imaging\n",
            "  14. BraTS21 ID\n",
            "  15. BraTS21 Segmentation Cohort\n",
            "  16. BraTS21 MGMT Cohort\n",
            "\n",
            "First 5 rows:\n",
            "              ID Sex  Age at MRI  WHO CNS Grade  \\\n",
            "0  UCSF-PDGM-004   M          66              4   \n",
            "1  UCSF-PDGM-005   F          80              4   \n",
            "2  UCSF-PDGM-007   M          70              4   \n",
            "3  UCSF-PDGM-008   M          70              4   \n",
            "4  UCSF-PDGM-009   F          68              4   \n",
            "\n",
            "  Final pathologic diagnosis (WHO 2021)    MGMT status MGMT index   1p/19q  \\\n",
            "0            Glioblastoma, IDH-wildtype       negative          0  unknown   \n",
            "1            Glioblastoma, IDH-wildtype  indeterminate    unknown  unknown   \n",
            "2            Glioblastoma, IDH-wildtype  indeterminate    unknown  unknown   \n",
            "3            Glioblastoma, IDH-wildtype       negative          0  unknown   \n",
            "4            Glioblastoma, IDH-wildtype       negative          0  unknown   \n",
            "\n",
            "        IDH  1-dead 0-alive      OS     EOR Biopsy prior to imaging  \\\n",
            "0  wildtype               1  1303.0     STR                      No   \n",
            "1  wildtype               1   274.0  biopsy                      No   \n",
            "2  wildtype               1   417.0     STR                      No   \n",
            "3  wildtype               1   185.0     STR                      No   \n",
            "4  wildtype               1   389.0     STR                      No   \n",
            "\n",
            "        BraTS21 ID BraTS21 Segmentation Cohort BraTS21 MGMT Cohort  \n",
            "0  BraTS2021_00097                    Training            Training  \n",
            "1              NaN                         NaN                 NaN  \n",
            "2  BraTS2021_00103                    Training                 NaN  \n",
            "3              NaN                         NaN                 NaN  \n",
            "4  BraTS2021_00049                    Training            Training  \n",
            "\n",
            "Checking for patient ID column...\n",
            "Potential ID columns: ['ID', 'IDH', 'BraTS21 ID']\n",
            "\n",
            "Sample values from 'ID':\n",
            "0    UCSF-PDGM-004\n",
            "1    UCSF-PDGM-005\n",
            "2    UCSF-PDGM-007\n",
            "3    UCSF-PDGM-008\n",
            "4    UCSF-PDGM-009\n",
            "5    UCSF-PDGM-010\n",
            "6    UCSF-PDGM-011\n",
            "7    UCSF-PDGM-012\n",
            "8    UCSF-PDGM-013\n",
            "9    UCSF-PDGM-014\n",
            "Name: ID, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV\n",
        "csv_path = \"https://raw.githubusercontent.com/Erdos-Projects/fall-2025-brain-tumor-diagnosis/refs/heads/main/Clinical-data/UCSF-PDGM-metadata_v5.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Get patient folders\n",
        "patient_folders = sorted([f for f in os.listdir(base_path)\n",
        "                         if os.path.isdir(os.path.join(base_path, f))])\n",
        "\n",
        "# Extract patient number and pad to 4 digits\n",
        "df['patient_num'] = df['ID'].str.extract(r'UCSF-PDGM-(\\d+)')[0]\n",
        "df['folder_name'] = 'UCSF-PDGM-' + df['patient_num'].str.zfill(4) + '_nifti'\n",
        "\n",
        "# Filter to only patients we have imaging data for\n",
        "df_with_imaging = df[df['folder_name'].isin(patient_folders)]\n",
        "\n",
        "print(f\"Total patients in CSV: {len(df)}\")\n",
        "print(f\"Total folders: {len(patient_folders)}\")\n",
        "print(f\"Matched patients (have both data and labels): {len(df_with_imaging)}\")\n",
        "\n",
        "# Check WHO Grade distribution\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"WHO Grade distribution:\")\n",
        "print(df_with_imaging['WHO CNS Grade'].value_counts().sort_index())\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Show first few matches\n",
        "print(f\"\\nFirst 5 matched patients:\")\n",
        "print(df_with_imaging[['ID', 'folder_name', 'WHO CNS Grade']].head())\n",
        "\n",
        "# Create labels dictionary\n",
        "labels_dict = dict(zip(df_with_imaging['folder_name'],\n",
        "                       df_with_imaging['WHO CNS Grade']))\n",
        "\n",
        "print(f\"\\nLabels dictionary created!\")\n",
        "print(f\"Example entries:\")\n",
        "for i, (folder, grade) in enumerate(list(labels_dict.items())[:5]):\n",
        "    print(f\"  {folder}: Grade {grade}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBb1vpZaoFFG",
        "outputId": "0d3c4e10-28c3-4c48-f026-d87d638f5cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patients in CSV: 501\n",
            "Total folders: 501\n",
            "Matched patients (have both data and labels): 501\n",
            "\n",
            "==================================================\n",
            "WHO Grade distribution:\n",
            "WHO CNS Grade\n",
            "2     56\n",
            "3     43\n",
            "4    402\n",
            "Name: count, dtype: int64\n",
            "==================================================\n",
            "\n",
            "First 5 matched patients:\n",
            "              ID           folder_name  WHO CNS Grade\n",
            "0  UCSF-PDGM-004  UCSF-PDGM-0004_nifti              4\n",
            "1  UCSF-PDGM-005  UCSF-PDGM-0005_nifti              4\n",
            "2  UCSF-PDGM-007  UCSF-PDGM-0007_nifti              4\n",
            "3  UCSF-PDGM-008  UCSF-PDGM-0008_nifti              4\n",
            "4  UCSF-PDGM-009  UCSF-PDGM-0009_nifti              4\n",
            "\n",
            "Labels dictionary created!\n",
            "Example entries:\n",
            "  UCSF-PDGM-0004_nifti: Grade 4\n",
            "  UCSF-PDGM-0005_nifti: Grade 4\n",
            "  UCSF-PDGM-0007_nifti: Grade 4\n",
            "  UCSF-PDGM-0008_nifti: Grade 4\n",
            "  UCSF-PDGM-0009_nifti: Grade 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 1. LOAD AND PREPARE LABELS\n",
        "# =============================================\n",
        "\n",
        "def load_labels(csv_path, base_path):\n",
        "    \"\"\"\n",
        "    Load labels from CSV and match to folder names\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to metadata CSV file\n",
        "        base_path: Path to folder containing patient folders\n",
        "\n",
        "    Returns:\n",
        "        labels_dict: Dictionary mapping folder_name -> grade (0, 1, 2)\n",
        "        patient_folders: List of patient folder names\n",
        "    \"\"\"\n",
        "    # Load CSV\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Get all patient folders\n",
        "    patient_folders = sorted([f for f in os.listdir(base_path)\n",
        "                             if os.path.isdir(os.path.join(base_path, f))])\n",
        "\n",
        "    # Match CSV IDs to folder names (pad numbers to 4 digits)\n",
        "    df['patient_num'] = df['ID'].str.extract(r'UCSF-PDGM-(\\d+)')[0]\n",
        "    df['folder_name'] = 'UCSF-PDGM-' + df['patient_num'].str.zfill(4) + '_nifti'\n",
        "\n",
        "    # Filter to patients with imaging data\n",
        "    df_matched = df[df['folder_name'].isin(patient_folders)]\n",
        "\n",
        "    # Convert WHO grades to class indices: Grade 2->0, Grade 3->1, Grade 4->2\n",
        "    grade_to_class = {2: 0, 3: 1, 4: 2}\n",
        "    df_matched['class_label'] = df_matched['WHO CNS Grade'].map(grade_to_class)\n",
        "\n",
        "    # Create labels dictionary\n",
        "    labels_dict = dict(zip(df_matched['folder_name'], df_matched['class_label']))\n",
        "\n",
        "    print(f\"Loaded {len(labels_dict)} patients with labels\")\n",
        "    print(f\"Class distribution: {df_matched['WHO CNS Grade'].value_counts().sort_index()}\")\n",
        "\n",
        "    return labels_dict, patient_folders\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# 2. DATASET CLASS\n",
        "# =============================================\n",
        "\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, base_path, patient_list, labels_dict, slice_idx=120, selected_modalities=None):\n",
        "        \"\"\"\n",
        "        Dataset for multi-channel 2D MRI slices\n",
        "\n",
        "        Args:\n",
        "            base_path: Path to folder containing patient folders\n",
        "            patient_list: List of patient folder names to include\n",
        "            labels_dict: Dictionary mapping folder_name -> label\n",
        "            slice_idx: Which slice to extract from 3D volume (default: 120)\n",
        "            selected_modalities: List of modality names to use. If None, uses all available.\n",
        "        \"\"\"\n",
        "        self.base_path = base_path\n",
        "        self.patient_list = patient_list\n",
        "        self.labels_dict = labels_dict\n",
        "        self.slice_idx = slice_idx\n",
        "\n",
        "        # Define modality order (consistent for all patients)\n",
        "        # Excluding non-NIfTI files and segmentation masks\n",
        "        #self.all_modalities = [\n",
        "            #'ADC', 'ASL', 'DTI_eddy_FA', 'DTI_eddy_L1', 'DTI_eddy_L2',\n",
        "            #'DTI_eddy_L3', 'DTI_eddy_MD', 'DWI', 'DWI_bias',\n",
        "            #'FLAIR', 'FLAIR_bias', 'SWI', 'SWI_bias',\n",
        "            #'T1', 'T1_bias', 'T1c', 'T1c_bias', 'T2', 'T2_bias'\n",
        "      #]\n",
        "        self.all_modalities = [\n",
        "            'ADC', 'FLAIR', 'T1', 'T1c', 'T2'\n",
        "        ]\n",
        "\n",
        "        # Use selected modalities or all\n",
        "        self.modalities = selected_modalities if selected_modalities else self.all_modalities\n",
        "\n",
        "        print(f\"Using {len(self.modalities)} modalities: {self.modalities[:3]}... (showing first 3)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient_folder = self.patient_list[idx]\n",
        "        patient_id = patient_folder.replace('_nifti', '')\n",
        "        patient_path = os.path.join(self.base_path, patient_folder)\n",
        "        seg_path = patient_path / f\"{patient_id}_tumor_segmentation.nii.gz\"\n",
        "\n",
        "        try:\n",
        "            seg_data = nib.load(str(seg_path)).get_fdata()\n",
        "            seg_bin = (seg_data > 0).astype(np.uint8)\n",
        "            tumor_area_by_slice = seg_bin.sum(axis=(0, 1))\n",
        "            z = int(np.argmax(tumor_area_by_slice)) if tumor_area_by_slice.max() > 0 else seg_bin.shape[2] // 2\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load segmentation for {patient_folder}: {e}\")\n",
        "\n",
        "        # Load all modalities and stack as channels\n",
        "        channels = []\n",
        "        for modality in self.modalities:\n",
        "            file_path = patient_path / f\"{patient_id}_{modality}.nii.gz\"\n",
        "\n",
        "            try:\n",
        "                # Load 3D volume\n",
        "                img_3d = nib.load(file_path).get_fdata().astype(np.float32)\n",
        "\n",
        "                # Extract 2D slice\n",
        "                if img_3d.shape[2] > self.slice_idx:\n",
        "                    img_2d = img_3d[:, :, self.slice_idx]\n",
        "                else:\n",
        "                    # If slice index out of range, use middle slice\n",
        "                    img_2d = img_3d[:, :, img_3d.shape[2]//2]\n",
        "\n",
        "                # Normalize to [0, 1]\n",
        "                img_min, img_max = img_2d.min(), img_2d.max()\n",
        "                if img_max > img_min:\n",
        "                    img_2d = (img_2d - img_min) / (img_max - img_min)\n",
        "                else:\n",
        "                    img_2d = np.zeros_like(img_2d)\n",
        "\n",
        "                channels.append(img_2d)\n",
        "\n",
        "            except Exception as e:\n",
        "                # If file missing or error, use zero-filled channel\n",
        "                print(f\"Warning: Could not load {modality} for {patient_folder}: {e}\")\n",
        "                channels.append(np.zeros((240, 240), dtype=np.float32))\n",
        "\n",
        "        # Stack channels: (num_channels, H, W)\n",
        "        img_tensor = np.stack(channels, axis=0)\n",
        "\n",
        "        # Get label\n",
        "        label = self.labels_dict[patient_folder]\n",
        "\n",
        "        return torch.tensor(img_tensor, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# 3. CNN MODEL\n",
        "# =============================================\n",
        "\n",
        "class CNN2D(nn.Module):\n",
        "    def __init__(self, n_channels=5, n_classes=3):\n",
        "        \"\"\"\n",
        "        2D CNN for multi-channel MRI classification\n",
        "\n",
        "        Args:\n",
        "            n_channels: Number of input channels (MRI modalities)\n",
        "            n_classes: Number of output classes (3 for WHO grades 2,3,4)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # Block 1: n_channels -> 32\n",
        "            nn.Conv2d(n_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 240x240 -> 120x120\n",
        "\n",
        "            # Block 2: 32 -> 64\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 120x120 -> 60x60\n",
        "\n",
        "            # Block 3: 64 -> 128\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 60x60 -> 30x30\n",
        "\n",
        "            # Block 4: 128 -> 256\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 30x30 -> 15x15\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(256 * 15 * 15, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# 4. TRAINING FUNCTION\n",
        "# =============================================\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, device='cuda'):\n",
        "    \"\"\"\n",
        "    Train the CNN model\n",
        "\n",
        "    Returns:\n",
        "        train_losses, val_losses, train_accs, val_accs\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100. * correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# 5. MAIN EXECUTION\n",
        "# =============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths\n",
        "    csv_path = \"/content/drive/MyDrive/UCSF-PDGM-metadata_v5-checkpoint.csv\"\n",
        "    base_path = \"/content/drive/Shareddrives/Fall2025BrainTumorDiagnosis/\"\n",
        "\n",
        "    # Load labels\n",
        "    labels_dict, all_patients = load_labels(csv_path, base_path)\n",
        "\n",
        "    # Get patients with labels\n",
        "    patients_with_labels = list(labels_dict.keys())\n",
        "\n",
        "    # Train/val split (80/20)\n",
        "    train_patients, val_patients = train_test_split(\n",
        "        patients_with_labels, test_size=0.2, random_state=42,\n",
        "        stratify=[labels_dict[p] for p in patients_with_labels]\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_patients)}, Val: {len(val_patients)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = MRIDataset(base_path, train_patients, labels_dict, slice_idx=120)\n",
        "    val_dataset = MRIDataset(base_path, val_patients, labels_dict, slice_idx=120)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    model = CNN2D(n_channels=5, n_classes=3)\n",
        "\n",
        "    # Calculate class weights to handle imbalance\n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "    all_labels = [labels_dict[p] for p in patients_with_labels]\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(all_labels),\n",
        "        y=all_labels\n",
        "    )\n",
        "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "    print(f\"\\nClass weights (to handle imbalance): {class_weights}\")\n",
        "    print(f\"  Grade 2 weight: {class_weights[0]:.2f}\")\n",
        "    print(f\"  Grade 3 weight: {class_weights[1]:.2f}\")\n",
        "    print(f\"  Grade 4 weight: {class_weights[2]:.2f}\")\n",
        "\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\nStarting training...\")\n",
        "    train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer,\n",
        "        num_epochs=20, device=device\n",
        "    )\n",
        "\n",
        "    # Plot results\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    ax1.plot(train_losses, label='Train Loss')\n",
        "    ax1.plot(val_losses, label='Val Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "\n",
        "    ax2.plot(train_accs, label='Train Acc')\n",
        "    ax2.plot(val_accs, label='Val Acc')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTraining complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "P1Tt-qGdm7gD",
        "outputId": "278d9a75-8f2c-4bfc-9515-f7744abbeb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 495 patients with labels\n",
            "Class distribution: WHO CNS Grade\n",
            "2     56\n",
            "3     43\n",
            "4    402\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train: 396, Val: 99\n",
            "Using 5 modalities: ['ADC', 'FLAIR', 'T1']... (showing first 3)\n",
            "Using 5 modalities: ['ADC', 'FLAIR', 'T1']... (showing first 3)\n",
            "\n",
            "Using device: cpu\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/20\n",
            "  Train Loss: 3.3239, Train Acc: 64.39%\n",
            "  Val Loss: 0.7581, Val Acc: 79.80%\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.1228, Train Acc: 71.97%\n",
            "  Val Loss: 0.6227, Val Acc: 79.80%\n",
            "Epoch 3/20\n",
            "  Train Loss: 0.7716, Train Acc: 76.77%\n",
            "  Val Loss: 0.7543, Val Acc: 79.80%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1982558841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     train_losses, val_losses, train_accs, val_accs = train_model(\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1982558841.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1982558841.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1701\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Balanced Input"
      ],
      "metadata": {
        "id": "WgodhQHJ6frt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# =============================================\n",
        "# 1. LOAD LABELS AND FILTER DUPLICATES\n",
        "# =============================================\n",
        "def load_labels(csv_path, base_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df[~df['ID'].str.contains('_FU')].copy()\n",
        "    patient_folders = sorted([f for f in os.listdir(base_path)\n",
        "                              if os.path.isdir(os.path.join(base_path, f))])\n",
        "    df['patient_num'] = df['ID'].str.extract(r'UCSF-PDGM-(\\d+)')[0]\n",
        "    df['folder_name'] = 'UCSF-PDGM-' + df['patient_num'].str.zfill(4) + '_nifti'\n",
        "    df_matched = df[df['folder_name'].isin(patient_folders)]\n",
        "    grade_to_class = {2:0, 3:1, 4:2}\n",
        "    df_matched['class_label'] = df_matched['WHO CNS Grade'].map(grade_to_class)\n",
        "    labels_dict = dict(zip(df_matched['folder_name'], df_matched['class_label']))\n",
        "    print(f\"Loaded {len(labels_dict)} patients with labels\")\n",
        "    print(\"Class distribution:\", df_matched['WHO CNS Grade'].value_counts().sort_index())\n",
        "    return labels_dict, patient_folders\n",
        "\n",
        "# =============================================\n",
        "# 2. DATASET CLASS WITH AUGMENTATION\n",
        "# =============================================\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, base_path, patient_list, labels_dict, modalities=None, augment=False):\n",
        "        self.base_path = base_path\n",
        "        self.patient_list = patient_list\n",
        "        self.labels_dict = labels_dict\n",
        "        self.modalities = modalities if modalities else ['ADC','FLAIR','T1','T1c','T2']\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder = self.patient_list[idx]\n",
        "        pid = folder.replace('_nifti','')\n",
        "        patient_path = os.path.join(self.base_path, folder)\n",
        "\n",
        "        # ----------------------\n",
        "        # Find z-slice with max tumor\n",
        "        # ----------------------\n",
        "        try:\n",
        "            seg_file = os.path.join(patient_path, f\"{pid}_tumor_segmentation.nii.gz\")\n",
        "            seg = nib.load(seg_file).get_fdata()\n",
        "            seg_bin = (seg>0).astype(np.uint8)\n",
        "            tumor_area_by_slice = seg_bin.sum(axis=(0,1))\n",
        "            z = int(np.argmax(tumor_area_by_slice)) if tumor_area_by_slice.max()>0 else seg.shape[2]//2\n",
        "        except:\n",
        "            z = 120\n",
        "\n",
        "        # ----------------------\n",
        "        # Load MRI slices\n",
        "        # ----------------------\n",
        "        channels = []\n",
        "        for mod in self.modalities:\n",
        "            img_file = os.path.join(patient_path, f\"{pid}_{mod}.nii.gz\")\n",
        "            try:\n",
        "                img = nib.load(img_file).get_fdata().astype(np.float32)\n",
        "                img2d = img[:,:,z] if img.shape[2] > z else img[:,:,img.shape[2]//2]\n",
        "                img2d = (img2d - img2d.min()) / (img2d.max()-img2d.min()) if img2d.max()>img2d.min() else np.zeros_like(img2d)\n",
        "            except:\n",
        "                img2d = np.zeros((240,240), dtype=np.float32)\n",
        "            channels.append(img2d)\n",
        "\n",
        "        img_tensor = torch.tensor(np.stack(channels, axis=0), dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels_dict[folder], dtype=torch.long)\n",
        "\n",
        "        # ----------------------\n",
        "        # Augmentation for minority classes\n",
        "        # ----------------------\n",
        "        if self.augment and label.item() in [0,1]:  # Only for grade 2 and 3\n",
        "            if np.random.rand() > 0.5:\n",
        "                img_tensor = img_tensor.flip(-1)  # Horizontal flip\n",
        "            if np.random.rand() > 0.5:\n",
        "                img_tensor = img_tensor.flip(-2)  # Vertical flip\n",
        "            k = np.random.choice([0,1,2,3])\n",
        "            img_tensor = torch.rot90(img_tensor, k, dims=[1,2])  # Random 90 deg rotation\n",
        "\n",
        "        return img_tensor, label\n",
        "\n",
        "# =============================================\n",
        "# 3. CNN MODEL\n",
        "# =============================================\n",
        "class CNN2D(nn.Module):\n",
        "    def __init__(self, n_channels=5, n_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(n_channels,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256*15*15,512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512,128), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(128,n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# =============================================\n",
        "# 4. TRAINING FUNCTION\n",
        "# =============================================\n",
        "def train_model_per_class(model, train_loader, val_loader, criterion, optimizer, device='cuda', epochs=20):\n",
        "    model = model.to(device)\n",
        "    n_classes = 3\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_acc_per_class': [], 'val_acc_per_class': [],\n",
        "        'train_loss_per_class': [], 'val_loss_per_class': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --------- TRAIN ---------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        class_correct = np.zeros(n_classes)\n",
        "        class_total = np.zeros(n_classes)\n",
        "        class_loss_sum = np.zeros(n_classes)\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            for c in range(n_classes):\n",
        "                idxs = (labels==c)\n",
        "                class_total[c] += idxs.sum().item()\n",
        "                class_correct[c] += (preds[idxs]==labels[idxs]).sum().item()\n",
        "                if idxs.sum()>0:\n",
        "                    class_loss_sum[c] += criterion(outputs[idxs], labels[idxs]).item()*idxs.sum().item()\n",
        "\n",
        "        train_loss = running_loss/total\n",
        "        train_acc = 100.*correct/total\n",
        "        train_acc_pc = 100.*class_correct/np.maximum(class_total,1)\n",
        "        train_loss_pc = class_loss_sum/np.maximum(class_total,1)\n",
        "\n",
        "        # --------- VALIDATION ---------\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        class_correct = np.zeros(n_classes)\n",
        "        class_total = np.zeros(n_classes)\n",
        "        class_loss_sum = np.zeros(n_classes)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()*labels.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                correct += preds.eq(labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                for c in range(n_classes):\n",
        "                    idxs = (labels==c)\n",
        "                    class_total[c] += idxs.sum().item()\n",
        "                    class_correct[c] += (preds[idxs]==labels[idxs]).sum().item()\n",
        "                    if idxs.sum()>0:\n",
        "                        class_loss_sum[c] += criterion(outputs[idxs], labels[idxs]).item()*idxs.sum().item()\n",
        "\n",
        "        val_loss = running_loss/total\n",
        "        val_acc = 100.*correct/total\n",
        "        val_acc_pc = 100.*class_correct/np.maximum(class_total,1)\n",
        "        val_loss_pc = class_loss_sum/np.maximum(class_total,1)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['train_acc_per_class'].append(train_acc_pc)\n",
        "        history['val_acc_per_class'].append(val_acc_pc)\n",
        "        history['train_loss_per_class'].append(train_loss_pc)\n",
        "        history['val_loss_per_class'].append(val_loss_pc)\n",
        "\n",
        "        # Print\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss {train_loss:.4f}, Acc {train_acc:.2f}% | Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%\")\n",
        "        for c in range(n_classes):\n",
        "            print(f\"  Grade {c+2}: Train Acc {train_acc_pc[c]:.2f}%, Loss {train_loss_pc[c]:.4f} | Val Acc {val_acc_pc[c]:.2f}%, Loss {val_loss_pc[c]:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# =============================================\n",
        "# 5. MAIN EXECUTION\n",
        "# =============================================\n",
        "if __name__==\"__main__\":\n",
        "    labels_dict, all_patients = load_labels(csv_path, base_path)\n",
        "    patients = list(labels_dict.keys())\n",
        "\n",
        "    train_patients, val_patients = train_test_split(\n",
        "        patients, test_size=0.2, random_state=42,\n",
        "        stratify=[labels_dict[p] for p in patients]\n",
        "    )\n",
        "    print(f\"Train {len(train_patients)}, Val {len(val_patients)}\")\n",
        "\n",
        "    # Compute sampling weights for WeightedRandomSampler\n",
        "    all_labels_train = [labels_dict[p] for p in train_patients]\n",
        "    class_sample_count = np.array([all_labels_train.count(c) for c in np.unique(all_labels_train)])\n",
        "    weights = 1. / class_sample_count\n",
        "    samples_weight = np.array([weights[c] for c in all_labels_train])\n",
        "    sampler = WeightedRandomSampler(weights=samples_weight, num_samples=len(samples_weight), replacement=True)\n",
        "\n",
        "    train_dataset = MRIDataset(base_path, train_patients, labels_dict, augment=True)\n",
        "    val_dataset = MRIDataset(base_path, val_patients, labels_dict, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = CNN2D(n_channels=5, n_classes=3)\n",
        "\n",
        "    # Class weights for loss\n",
        "    all_labels = [labels_dict[p] for p in patients]\n",
        "    class_weights = torch.FloatTensor(compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train\n",
        "    history = train_model_per_class(model, train_loader, val_loader, criterion, optimizer, device=device, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88PaO7Fs4bmN",
        "outputId": "a65307dd-d756-4223-83e0-c7838d2f82c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 495 patients with labels\n",
            "Class distribution: WHO CNS Grade\n",
            "2     56\n",
            "3     43\n",
            "4    396\n",
            "Name: count, dtype: int64\n",
            "Train 396, Val 99\n",
            "Epoch 1/20 | Train Loss 3.9023, Acc 38.13% | Val Loss 2.1232, Acc 11.11%\n",
            "  Grade 2: Train Acc 41.09%, Loss 3.9063 | Val Acc 100.00%, Loss 0.1742\n",
            "  Grade 3: Train Acc 56.72%, Loss 2.9159 | Val Acc 0.00%, Loss 1.9050\n",
            "  Grade 4: Train Acc 16.54%, Loss 11.2427 | Val Acc 0.00%, Loss 3.6459\n",
            "Epoch 2/20 | Train Loss 1.4194, Acc 37.63% | Val Loss 1.8829, Acc 9.09%\n",
            "  Grade 2: Train Acc 44.92%, Loss 1.5234 | Val Acc 0.00%, Loss 1.5574\n",
            "  Grade 3: Train Acc 55.30%, Loss 1.0483 | Val Acc 100.00%, Loss 0.4047\n",
            "  Grade 4: Train Acc 15.75%, Loss 3.7271 | Val Acc 0.00%, Loss 3.1312\n",
            "Epoch 3/20 | Train Loss 1.0473, Acc 42.68% | Val Loss 1.4654, Acc 16.16%\n",
            "  Grade 2: Train Acc 49.30%, Loss 1.0346 | Val Acc 54.55%, Loss 0.8617\n",
            "  Grade 3: Train Acc 52.71%, Loss 0.8612 | Val Acc 33.33%, Loss 0.7835\n",
            "  Grade 4: Train Acc 24.80%, Loss 2.4763 | Val Acc 8.86%, Loss 2.3475\n",
            "Epoch 4/20 | Train Loss 0.9719, Acc 41.92% | Val Loss 1.4136, Acc 10.10%\n",
            "  Grade 2: Train Acc 50.40%, Loss 0.9377 | Val Acc 0.00%, Loss 0.9306\n",
            "  Grade 3: Train Acc 60.43%, Loss 0.7120 | Val Acc 100.00%, Loss 0.6480\n",
            "  Grade 4: Train Acc 14.39%, Loss 2.8292 | Val Acc 1.27%, Loss 2.2422\n",
            "Epoch 5/20 | Train Loss 0.9537, Acc 32.83% | Val Loss 1.3427, Acc 12.12%\n",
            "  Grade 2: Train Acc 26.19%, Loss 1.0937 | Val Acc 36.36%, Loss 0.9025\n",
            "  Grade 3: Train Acc 64.39%, Loss 0.6698 | Val Acc 44.44%, Loss 0.7655\n",
            "  Grade 4: Train Acc 8.70%, Loss 2.4047 | Val Acc 5.06%, Loss 2.0290\n",
            "Epoch 6/20 | Train Loss 0.9100, Acc 38.64% | Val Loss 1.2528, Acc 13.13%\n",
            "  Grade 2: Train Acc 30.83%, Loss 1.0124 | Val Acc 27.27%, Loss 0.9241\n",
            "  Grade 3: Train Acc 73.61%, Loss 0.6034 | Val Acc 55.56%, Loss 0.8063\n",
            "  Grade 4: Train Acc 5.04%, Loss 3.1315 | Val Acc 6.33%, Loss 1.8065\n",
            "Epoch 7/20 | Train Loss 0.8685, Acc 38.64% | Val Loss 1.1627, Acc 27.27%\n",
            "  Grade 2: Train Acc 43.88%, Loss 0.8545 | Val Acc 36.36%, Loss 1.0044\n",
            "  Grade 3: Train Acc 68.60%, Loss 0.6479 | Val Acc 33.33%, Loss 0.9720\n",
            "  Grade 4: Train Acc 6.62%, Loss 2.3841 | Val Acc 25.32%, Loss 1.4701\n",
            "Epoch 8/20 | Train Loss 0.9638, Acc 41.16% | Val Loss 1.2027, Acc 19.19%\n",
            "  Grade 2: Train Acc 50.69%, Loss 0.9191 | Val Acc 9.09%, Loss 1.0895\n",
            "  Grade 3: Train Acc 58.82%, Loss 0.7683 | Val Acc 88.89%, Loss 0.7888\n",
            "  Grade 4: Train Acc 15.04%, Loss 2.3779 | Val Acc 12.66%, Loss 1.5804\n",
            "Epoch 9/20 | Train Loss 0.8290, Acc 40.15% | Val Loss 1.2466, Acc 17.17%\n",
            "  Grade 2: Train Acc 48.51%, Loss 0.8298 | Val Acc 54.55%, Loss 0.9041\n",
            "  Grade 3: Train Acc 61.76%, Loss 0.6488 | Val Acc 22.22%, Loss 0.9742\n",
            "  Grade 4: Train Acc 7.94%, Loss 2.2632 | Val Acc 11.39%, Loss 1.6793\n",
            "Epoch 10/20 | Train Loss 0.8510, Acc 43.94% | Val Loss 1.3068, Acc 22.22%\n",
            "  Grade 2: Train Acc 50.00%, Loss 0.8740 | Val Acc 81.82%, Loss 0.7800\n",
            "  Grade 3: Train Acc 66.18%, Loss 0.6620 | Val Acc 22.22%, Loss 0.9715\n",
            "  Grade 4: Train Acc 15.15%, Loss 2.3923 | Val Acc 13.92%, Loss 1.9077\n",
            "Epoch 11/20 | Train Loss 0.8748, Acc 42.68% | Val Loss 1.1833, Acc 21.21%\n",
            "  Grade 2: Train Acc 51.49%, Loss 0.8654 | Val Acc 81.82%, Loss 0.8299\n",
            "  Grade 3: Train Acc 69.49%, Loss 0.6676 | Val Acc 33.33%, Loss 0.9621\n",
            "  Grade 4: Train Acc 12.50%, Loss 2.2056 | Val Acc 11.39%, Loss 1.6042\n",
            "Epoch 12/20 | Train Loss 0.8528, Acc 43.69% | Val Loss 1.1658, Acc 37.37%\n",
            "  Grade 2: Train Acc 58.14%, Loss 0.7822 | Val Acc 63.64%, Loss 0.9362\n",
            "  Grade 3: Train Acc 53.66%, Loss 0.7419 | Val Acc 22.22%, Loss 0.9657\n",
            "  Grade 4: Train Acc 22.22%, Loss 1.8319 | Val Acc 35.44%, Loss 1.4737\n",
            "Epoch 13/20 | Train Loss 0.7790, Acc 49.75% | Val Loss 1.2954, Acc 18.18%\n",
            "  Grade 2: Train Acc 54.69%, Loss 0.8481 | Val Acc 0.00%, Loss 1.1455\n",
            "  Grade 3: Train Acc 74.50%, Loss 0.5859 | Val Acc 88.89%, Loss 0.6735\n",
            "  Grade 4: Train Acc 13.45%, Loss 2.2777 | Val Acc 12.66%, Loss 1.8443\n",
            "Epoch 14/20 | Train Loss 0.8259, Acc 47.47% | Val Loss 1.1774, Acc 27.27%\n",
            "  Grade 2: Train Acc 45.38%, Loss 0.9167 | Val Acc 36.36%, Loss 1.0326\n",
            "  Grade 3: Train Acc 77.27%, Loss 0.5711 | Val Acc 33.33%, Loss 0.8621\n",
            "  Grade 4: Train Acc 20.15%, Loss 2.1121 | Val Acc 25.32%, Loss 1.5406\n",
            "Epoch 15/20 | Train Loss 0.8341, Acc 47.22% | Val Loss 1.1240, Acc 17.17%\n",
            "  Grade 2: Train Acc 48.44%, Loss 0.9473 | Val Acc 9.09%, Loss 0.9960\n",
            "  Grade 3: Train Acc 74.47%, Loss 0.6021 | Val Acc 44.44%, Loss 0.8471\n",
            "  Grade 4: Train Acc 15.75%, Loss 2.2533 | Val Acc 15.19%, Loss 1.4282\n",
            "Epoch 16/20 | Train Loss 0.8217, Acc 42.17% | Val Loss 1.2045, Acc 23.23%\n",
            "  Grade 2: Train Acc 61.27%, Loss 0.7182 | Val Acc 63.64%, Loss 0.8373\n",
            "  Grade 3: Train Acc 54.33%, Loss 0.7493 | Val Acc 22.22%, Loss 1.1481\n",
            "  Grade 4: Train Acc 8.66%, Loss 2.0872 | Val Acc 17.72%, Loss 1.5110\n",
            "Epoch 17/20 | Train Loss 0.8186, Acc 43.43% | Val Loss 1.1874, Acc 30.30%\n",
            "  Grade 2: Train Acc 43.20%, Loss 0.8915 | Val Acc 90.91%, Loss 0.7738\n",
            "  Grade 3: Train Acc 78.03%, Loss 0.5975 | Val Acc 22.22%, Loss 1.1104\n",
            "  Grade 4: Train Acc 10.79%, Loss 2.1149 | Val Acc 22.78%, Loss 1.5339\n",
            "Epoch 18/20 | Train Loss 0.7677, Acc 51.77% | Val Loss 1.3019, Acc 24.24%\n",
            "  Grade 2: Train Acc 54.62%, Loss 0.7569 | Val Acc 54.55%, Loss 0.7742\n",
            "  Grade 3: Train Acc 73.47%, Loss 0.6416 | Val Acc 33.33%, Loss 1.0721\n",
            "  Grade 4: Train Acc 21.85%, Loss 1.8902 | Val Acc 18.99%, Loss 1.8663\n",
            "Epoch 19/20 | Train Loss 0.9098, Acc 39.14% | Val Loss 1.3080, Acc 7.07%\n",
            "  Grade 2: Train Acc 33.61%, Loss 1.0200 | Val Acc 54.55%, Loss 0.8368\n",
            "  Grade 3: Train Acc 76.39%, Loss 0.6774 | Val Acc 11.11%, Loss 0.8365\n",
            "  Grade 4: Train Acc 3.76%, Loss 2.1999 | Val Acc 0.00%, Loss 1.9668\n",
            "Epoch 20/20 | Train Loss 0.7955, Acc 51.52% | Val Loss 1.5417, Acc 14.14%\n",
            "  Grade 2: Train Acc 59.73%, Loss 0.7285 | Val Acc 81.82%, Loss 0.5549\n",
            "  Grade 3: Train Acc 74.40%, Loss 0.6652 | Val Acc 22.22%, Loss 2.0497\n",
            "  Grade 4: Train Acc 18.03%, Loss 2.1755 | Val Acc 3.80%, Loss 1.8998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#balanced input + smooth out the class weights loss + k-fold stratified + metadata in the CNN model\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# =============================================\n",
        "# 1. LOAD LABELS AND METADATA\n",
        "# =============================================\n",
        "def load_labels(csv_path, base_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df[~df['ID'].str.contains('_FU')].copy()\n",
        "    patient_folders = sorted([\n",
        "        f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))\n",
        "    ])\n",
        "    df['patient_num'] = df['ID'].str.extract(r'UCSF-PDGM-(\\d+)')[0]\n",
        "    df['folder_name'] = 'UCSF-PDGM-' + df['patient_num'].str.zfill(4) + '_nifti'\n",
        "    df_matched = df[df['folder_name'].isin(patient_folders)]\n",
        "\n",
        "    grade_to_class = {2: 0, 3: 1, 4: 2}\n",
        "    df_matched['class_label'] = df_matched['WHO CNS Grade'].map(grade_to_class)\n",
        "\n",
        "    # Encode metadata\n",
        "    df_matched['Sex_encoded'] = df_matched['Sex'].apply(lambda x: 1 if x == \"M\" else 0)\n",
        "    df_matched['IDH_encoded'] = df_matched['IDH'].apply(lambda x: 0 if x == \"wildtype\" else 1)\n",
        "    df_matched['Codeletion_encoded'] = df_matched['1p/19q'].apply(lambda x: 1 if x == \"co-deleted\" else 0)\n",
        "    df_matched['Age_scaled'] = StandardScaler().fit_transform(df_matched[['Age at MRI']])\n",
        "\n",
        "    labels_dict = dict(zip(df_matched['folder_name'], df_matched['class_label']))\n",
        "    metadata_dict = df_matched.set_index('folder_name')[[\n",
        "        'Age_scaled', 'Sex_encoded', 'IDH_encoded', 'Codeletion_encoded'\n",
        "    ]].to_dict(orient='index')\n",
        "\n",
        "    print(f\"Loaded {len(labels_dict)} patients with labels\")\n",
        "    print(\"Class distribution:\")\n",
        "    print(df_matched['WHO CNS Grade'].value_counts().sort_index())\n",
        "\n",
        "    return labels_dict, metadata_dict, patient_folders\n",
        "\n",
        "# =============================================\n",
        "# 2. DATASET CLASS\n",
        "# =============================================\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, base_path, patient_list, labels_dict, metadata_dict, modalities=None, augment=False):\n",
        "        self.base_path = base_path\n",
        "        self.patient_list = patient_list\n",
        "        self.labels_dict = labels_dict\n",
        "        self.metadata_dict = metadata_dict\n",
        "        self.modalities = modalities if modalities else ['ADC', 'FLAIR', 'T1', 'T1c', 'T2']\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder = self.patient_list[idx]\n",
        "        pid = folder.replace('_nifti', '')\n",
        "        patient_path = os.path.join(self.base_path, folder)\n",
        "\n",
        "        # --- Find z-slice with max tumor ---\n",
        "        try:\n",
        "            seg_file = os.path.join(patient_path, f\"{pid}_tumor_segmentation.nii.gz\")\n",
        "            seg = nib.load(seg_file).get_fdata()\n",
        "            seg_bin = (seg > 0).astype(np.uint8)\n",
        "            tumor_area_by_slice = seg_bin.sum(axis=(0, 1))\n",
        "            z = int(np.argmax(tumor_area_by_slice)) if tumor_area_by_slice.max() > 0 else seg.shape[2] // 2\n",
        "        except:\n",
        "            z = 120\n",
        "\n",
        "        # --- Load MRI slices ---\n",
        "        channels = []\n",
        "        for mod in self.modalities:\n",
        "            img_file = os.path.join(patient_path, f\"{pid}_{mod}.nii.gz\")\n",
        "            try:\n",
        "                img = nib.load(img_file).get_fdata().astype(np.float32)\n",
        "                img2d = img[:, :, z] if img.shape[2] > z else img[:, :, img.shape[2] // 2]\n",
        "                img2d = (img2d - img2d.min()) / (img2d.max() - img2d.min()) if img2d.max() > img2d.min() else np.zeros_like(img2d)\n",
        "            except:\n",
        "                img2d = np.zeros((240, 240), dtype=np.float32)\n",
        "            channels.append(img2d)\n",
        "\n",
        "        img_tensor = torch.tensor(np.stack(channels, axis=0), dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels_dict[folder], dtype=torch.long)\n",
        "\n",
        "        # --- Metadata ---\n",
        "        meta_values = list(self.metadata_dict[folder].values())\n",
        "        meta_tensor = torch.tensor(meta_values, dtype=torch.float32)\n",
        "\n",
        "        # --- Augmentation ---\n",
        "        if self.augment and label.item() in [0, 1]:\n",
        "            if np.random.rand() > 0.5:\n",
        "                img_tensor = img_tensor.flip(-1)\n",
        "            if np.random.rand() > 0.5:\n",
        "                img_tensor = img_tensor.flip(-2)\n",
        "            k = np.random.choice([0, 1, 2, 3])\n",
        "            img_tensor = torch.rot90(img_tensor, k, dims=[1, 2])\n",
        "\n",
        "        return img_tensor, meta_tensor, label\n",
        "\n",
        "# =============================================\n",
        "# 3. CNN + METADATA MODEL\n",
        "# =============================================\n",
        "class CNN2D_Meta(nn.Module):\n",
        "    def __init__(self, n_channels=5, n_meta=4, n_classes=3):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(n_channels, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.flatten_dim = 256 * 15 * 15\n",
        "        self.fc_img = nn.Sequential(\n",
        "            nn.Linear(self.flatten_dim, 512), nn.ReLU(), nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc_meta = nn.Sequential(\n",
        "            nn.Linear(n_meta, 32), nn.ReLU(), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.fc_combined = nn.Sequential(\n",
        "            nn.Linear(512 + 32, 128), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_meta):\n",
        "        x_img = self.conv_layers(x_img)\n",
        "        x_img = torch.flatten(x_img, 1)\n",
        "        x_img = self.fc_img(x_img)\n",
        "        x_meta = self.fc_meta(x_meta)\n",
        "        x = torch.cat((x_img, x_meta), dim=1)\n",
        "        out = self.fc_combined(x)\n",
        "        return out\n",
        "\n",
        "# =============================================\n",
        "# 4. TRAINING FUNCTION WITH PER-GRADE METRICS\n",
        "# =============================================\n",
        "def train_model_per_class(model, train_loader, val_loader, criterion, optimizer, device='cuda', epochs=20):\n",
        "    model.to(device)\n",
        "    n_classes = 3\n",
        "    for epoch in range(epochs):\n",
        "        # ---------- TRAIN ----------\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "        class_correct, class_total, class_loss_sum = np.zeros(n_classes), np.zeros(n_classes), np.zeros(n_classes)\n",
        "\n",
        "        for imgs, metas, labels in train_loader:\n",
        "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs, metas)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            for c in range(n_classes):\n",
        "                idx = (labels == c)\n",
        "                class_total[c] += idx.sum().item()\n",
        "                if idx.sum() > 0:\n",
        "                    class_correct[c] += (preds[idx] == labels[idx]).sum().item()\n",
        "                    class_loss_sum[c] += criterion(outputs[idx], labels[idx]).item() * idx.sum().item()\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = 100. * correct / total\n",
        "        train_acc_pc = 100. * class_correct / np.maximum(class_total, 1)\n",
        "        train_loss_pc = class_loss_sum / np.maximum(class_total, 1)\n",
        "\n",
        "        # ---------- VALIDATION ----------\n",
        "        model.eval()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "        class_correct, class_total, class_loss_sum = np.zeros(n_classes), np.zeros(n_classes), np.zeros(n_classes)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, metas, labels in val_loader:\n",
        "                imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
        "                outputs = model(imgs, metas)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * labels.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                correct += preds.eq(labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                for c in range(n_classes):\n",
        "                    idx = (labels == c)\n",
        "                    class_total[c] += idx.sum().item()\n",
        "                    if idx.sum() > 0:\n",
        "                        class_correct[c] += (preds[idx] == labels[idx]).sum().item()\n",
        "                        class_loss_sum[c] += criterion(outputs[idx], labels[idx]).item() * idx.sum().item()\n",
        "\n",
        "        val_loss = running_loss / total\n",
        "        val_acc = 100. * correct / total\n",
        "        val_acc_pc = 100. * class_correct / np.maximum(class_total, 1)\n",
        "        val_loss_pc = class_loss_sum / np.maximum(class_total, 1)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss {train_loss:.4f}, Acc {train_acc:.2f}% | Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%\")\n",
        "        for c in range(n_classes):\n",
        "            print(f\"  Grade {c+2}: Train Acc {train_acc_pc[c]:.2f}%, Loss {train_loss_pc[c]:.4f} | Val Acc {val_acc_pc[c]:.2f}%, Loss {val_loss_pc[c]:.4f}\")\n",
        "\n",
        "# =============================================\n",
        "# 5. MAIN EXECUTION WITH STRATIFIED K-FOLD\n",
        "# =============================================\n",
        "if __name__ == \"__main__\":\n",
        "    labels_dict, metadata_dict, all_patients = load_labels(csv_path, base_path)\n",
        "    patients = list(labels_dict.keys())\n",
        "    labels = np.array([labels_dict[p] for p in patients])\n",
        "\n",
        "    # --- Calibrate class weights ---\n",
        "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
        "    class_weights = 1.0 / np.sqrt(class_counts)\n",
        "    class_weights = class_weights / class_weights.sum() * len(unique_classes)\n",
        "    print(\"Adjusted class weights:\", class_weights)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # --- Stratified K-Fold ---\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(patients, labels)):\n",
        "        print(f\"\\n===== Fold {fold+1} / 5 =====\")\n",
        "\n",
        "        train_patients = [patients[i] for i in train_idx]\n",
        "        val_patients = [patients[i] for i in val_idx]\n",
        "\n",
        "        all_labels_train = [labels_dict[p] for p in train_patients]\n",
        "        class_sample_count = np.array([all_labels_train.count(c) for c in np.unique(all_labels_train)])\n",
        "        weights = 1.0 / np.sqrt(class_sample_count)\n",
        "        samples_weight = np.array([weights[c] for c in all_labels_train])\n",
        "        sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
        "\n",
        "        train_dataset = MRIDataset(base_path, train_patients, labels_dict, metadata_dict, augment=True)\n",
        "        val_dataset = MRIDataset(base_path, val_patients, labels_dict, metadata_dict, augment=False)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler, num_workers=2)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "        model = CNN2D_Meta(n_channels=5, n_meta=4, n_classes=3).to(device)\n",
        "        weight_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        train_model_per_class(model, train_loader, val_loader, criterion, optimizer, device=device, epochs=20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn1IciEiPlg9",
        "outputId": "11b2be36-0544-4d43-fe97-27666be74534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 495 patients with labels\n",
            "Class distribution:\n",
            "WHO CNS Grade\n",
            "2     56\n",
            "3     43\n",
            "4    396\n",
            "Name: count, dtype: int64\n",
            "Adjusted class weights: [1.19177886 1.36005184 0.44816929]\n",
            "\n",
            "===== Fold 1 / 5 =====\n",
            "Epoch 1/20 | Train Loss 5.2270, Acc 40.91% | Val Loss 1.7112, Acc 24.24%\n",
            "  Grade 2: Train Acc 27.59%, Loss 6.0375 | Val Acc 63.64%, Loss 0.7162\n",
            "  Grade 3: Train Acc 36.36%, Loss 6.1647 | Val Acc 0.00%, Loss 1.2538\n",
            "  Grade 4: Train Acc 47.41%, Loss 3.8774 | Val Acc 21.25%, Loss 2.0504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Balanced Input and Taking Multiple Slices"
      ],
      "metadata": {
        "id": "v6kSB5496tM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for g in optimizer.param_groups:\n",
        "    g['lr'] = g['lr'] * 0.3\n",
        "print(\"Reduced LR:\", optimizer.param_groups[0]['lr'])"
      ],
      "metadata": {
        "id": "t6Dbzls8zGv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save the model to Google drive"
      ],
      "metadata": {
        "id": "ltr82i4BRwQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8s-tRGPnRtHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/brain_tumor_cnn/model_final.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Model saved to:\", save_path)"
      ],
      "metadata": {
        "id": "g9Y1YK_8Rtlu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}