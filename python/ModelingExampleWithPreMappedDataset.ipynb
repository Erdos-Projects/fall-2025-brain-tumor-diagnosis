{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append(\"./python\")\n",
    "from MRIPreMappedDataset import SampleMapper, MRIPreMappedDataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed95d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Model Definition\n",
    "# =============================\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(self, n_channels=1, n_classes=1):\n",
    "        \"\"\"\n",
    "        2D CNN for multi-channel MRI classification\n",
    "\n",
    "        Args:\n",
    "            n_channels: Number of input channels (MRI modalities)\n",
    "            n_classes: Tumor/No-Tumor binary classification\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1: n_channels -> 32\n",
    "            nn.Conv2d(n_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 240x240 -> 120x120\n",
    "\n",
    "            # Block 2: 32 -> 64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 120x120 -> 60x60\n",
    "\n",
    "            # Block 3: 64 -> 128\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 60x60 -> 30x30\n",
    "\n",
    "            # Block 4: 128 -> 256\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 30x30 -> 15x15\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 15 * 15, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# =============================================\n",
    "# Training routine\n",
    "# =============================================\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the CNN model\n",
    "\n",
    "    Returns:\n",
    "        train_losses, val_losses, train_accs, val_accs\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235054c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct patient list and related tumor grade\n",
    "data_dir = \"../data\"\n",
    "full_patient_list = []\n",
    "for patient in listdir(data_dir):\n",
    "    if \"FU\" in patient:\n",
    "        continue\n",
    "    full_patient_list.append(patient.replace(\"_nifti\", \"\"))\n",
    "\n",
    "tumor_grade = []\n",
    "meta_data = pd.read_csv(\"../processed-data/UCSF-PDGM-metadata_v5.csv\")\n",
    "meta_data[\"ID\"] = meta_data[\"ID\"].apply(lambda x: \"-\".join(x.split(\"-\")[:-1]) + \"-\" + x.split(\"-\")[-1].rjust(4, \"0\"))\n",
    "grade_key = \"WHO CNS Grade\"\n",
    "for patient_id in full_patient_list:\n",
    "    grade = meta_data[grade_key].loc[meta_data[\"ID\"] == patient_id]\n",
    "    tumor_grade.append(grade.values[0])\n",
    "print(len(full_patient_list), len(tumor_grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54900d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "patient_id_list = full_patient_list\n",
    "full_sample_map = SampleMapper(data_dir,\n",
    "                               patient_id_list=patient_id_list,\n",
    "                               samples_per_patient_per_label=2,\n",
    "                               min_relative_brain_area_per_sample=.25,\n",
    "                               mri_axis=2,\n",
    "                               random_seed=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the splits are properly stratified with respect to tumor class.\n",
    "train_patients, test_patients, train_tumor, test_tumor = train_test_split(full_patient_list, tumor_grade, test_size=0.2, random_state=360, stratify=tumor_grade)\n",
    "print(\"N training patients:\", len(train_patients))\n",
    "print(\"N test patients:\", len(test_patients))\n",
    "print(\"N train tumor grade:\", len(train_tumor))\n",
    "classes, counts = np.unique(train_tumor, return_counts=True)\n",
    "print(\"\\tTraining tumor classes:\", classes)\n",
    "print(\"\\tTraining tumor class proportion:\", counts/counts.sum())\n",
    "print(\"N test tumor grade:\", len(test_tumor))\n",
    "classes, counts = np.unique(test_tumor, return_counts=True)\n",
    "print(\"\\tTest tumor classes:\", classes) \n",
    "print(\"\\tTest tumor class proportion:\", counts/counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bdd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "full_data_map = full_sample_map.data_map\n",
    "mri_axis = full_sample_map.mri_axis\n",
    "\n",
    "## Separate the training data into 4 batches for 4Fold Cross validation.\n",
    "n_splits = 4\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=360)\n",
    "\n",
    "# Dataset sampling parameters\n",
    "selected_modalities = [\"T1\"]\n",
    "\n",
    "# DataLoader parameters\n",
    "batch_size = 32\n",
    "\n",
    "n_epochs = 20\n",
    "models = []\n",
    "loss = {}\n",
    "accuracy = {}\n",
    "\n",
    "start_time = time.time()\n",
    "for i, (train_indices, validation_indices) in enumerate(skf.split(train_patients, train_tumor)):\n",
    "    # Multiple slices are taken from the MRI of a single patient_id. \n",
    "    # Dataset indices for these slices are pulled from the get_indices_from_patient_list method.\n",
    "    print(\"Currently running through fold\", i)\n",
    "\n",
    "    training_patients = np.array(full_patient_list)[train_indices]\n",
    "    validation_patients = np.array(full_patient_list)[validation_indices]\n",
    "\n",
    "    training_data_map = full_data_map.loc[full_data_map[\"patient_id\"].isin(training_patients)]\n",
    "    validation_data_map = full_data_map.loc[full_data_map[\"patient_id\"].isin(validation_patients)]\n",
    "\n",
    "    \n",
    "    print(\"Loading the training dataset...\")\n",
    "    load_start_time = time.time()\n",
    "    training_dataset = MRIPreMappedDataset(data_dir,\n",
    "                                           training_data_map,\n",
    "                                           selected_modalities=selected_modalities,\n",
    "                                           mri_axis=mri_axis)\n",
    "    print(\"Finished loading the training dataset!\")\n",
    "    print(f\"Time to load: {(time.time() - load_start_time)/60} minutes\")\n",
    "    load_start_time = time.time()\n",
    "    print(\"Loading the validation dataset...\")\n",
    "    validation_dataset = MRIPreMappedDataset(data_dir,\n",
    "                                               validation_data_map,\n",
    "                                               selected_modalities=selected_modalities,\n",
    "                                               mri_axis=mri_axis)\n",
    "    print(\"Finished loading the validation dataset!\")\n",
    "    print(f\"Time to load: {(time.time() - load_start_time)/60:.2f} minutes\")\n",
    "    print(\"N training samples:\", len(training_dataset))\n",
    "    print(\"N validation samples:\" , len(validation_dataset))\n",
    "\n",
    "    print(\"Building dataloaders...\")\n",
    "    training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Building model and setting the criterion.\")\n",
    "    model = CNN2D(n_channels=len(selected_modalities), n_classes=1)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print(\"Training!\")\n",
    "    load_start_time = time.time()\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, training_loader, validation_loader, criterion, optimizer,\n",
    "        num_epochs=n_epochs, device=device\n",
    "    )\n",
    "    print(\"Finished training!\")\n",
    "    print(\"Time to train:\", (time.time() - load_start_time) / 60, \"minutes\")\n",
    "    print(\"Losses:\")\n",
    "    print(\"\\tTraining:\", train_losses)\n",
    "    print(\"\\tValidation:\", val_losses)\n",
    "    print(\"Accuracy:\")\n",
    "    loss[i] = [train_losses, val_losses]\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2,2)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    _x = loss[i][0]\n",
    "    _y = loss[i][1]\n",
    "    ax.plot(range(len(_x)),_x)\n",
    "    ax.plot(range(len(_y)),_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5580ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_2025_brain_tumor_diagnosis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
